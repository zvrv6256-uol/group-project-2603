{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0969ae30",
   "metadata": {},
   "source": [
    "# MATH2603 Lab — Artificial Neural Networks (Part I)\n",
    "## Perceptrons · Activation functions · Decision boundaries · Overfitting vs underfitting\n",
    "\n",
    "This practical accompanies **ANN – Part I**. You will run small computational experiments to understand:\n",
    "- what a **perceptron** can (and cannot) represent\n",
    "- why **nonlinear activation functions** matter\n",
    "- how a small neural network can learn **nonlinear decision boundaries**\n",
    "- the difference between **underfitting** and **overfitting**\n",
    "\n",
    "> **How to run:** click a code cell and press **Shift + Enter**.\n",
    "\n",
    "### \n",
    "- **Part A :** Perceptron as a linear classifier (and why XOR fails)\n",
    "- **Part B :** Soft perceptron with sigmoid; probability view\n",
    "- **Part C :** Small neural network (MLP) learns nonlinear boundaries\n",
    "- **Part D :** Overfitting vs underfitting with train/test split\n",
    "- **Wrap-up :** Reflection questions (useful for portfolio)\n",
    "\n",
    "\n",
    "> **Important:** If you see `NameError` (a function is not defined), you probably ran cells out of order. Use **Run All** once, then work top-to-bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c96712e",
   "metadata": {},
   "source": [
    "## 0) Setup check (run first)\n",
    "\n",
    "If you get `ModuleNotFoundError`, install packages from a terminal (Anaconda Prompt / command line):\n",
    "\n",
    "```bash\n",
    "pip install numpy matplotlib scikit-learn\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b8cb6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db62303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.1\n",
      "scikit-learn: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    print(\"scikit-learn:\", sklearn.__version__)\n",
    "except Exception as e:\n",
    "    print(\"scikit-learn not available yet:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7dcdc9",
   "metadata": {},
   "source": [
    "## Helper functions (plots + datasets)\n",
    "\n",
    "We will work with **2D toy datasets** so we can *see* decision boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa3c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_linearly_separable(n=200, noise=0.25, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.normal(size=(n, 2))\n",
    "    y = (X[:, 0] + X[:, 1] > 0).astype(int)  # true boundary\n",
    "    X = X + noise * rng.normal(size=X.shape)  # jitter inputs\n",
    "    return X, y\n",
    "\n",
    "def make_xor(n=200, noise=0.2, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.uniform(-1, 1, size=(n, 2))\n",
    "    y = ((X[:, 0] > 0) ^ (X[:, 1] > 0)).astype(int)\n",
    "    X = X + noise * rng.normal(size=X.shape)\n",
    "    return X, y\n",
    "\n",
    "def plot_points(X, y, title=\"Dataset\"):\n",
    "    plt.figure(figsize=(5.5, 5))\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], s=20, label=\"Class 0\")\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], s=20, label=\"Class 1\")\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_decision_boundary(predict_fn, X, y, title=\"Decision boundary\", grid_step=0.02):\n",
    "    x_min, x_max = X[:,0].min()-0.6, X[:,0].max()+0.6\n",
    "    y_min, y_max = X[:,1].min()-0.6, X[:,1].max()+0.6\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, grid_step),\n",
    "                         np.arange(y_min, y_max, grid_step))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    zz = predict_fn(grid).reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(5.5, 5))\n",
    "    plt.contourf(xx, yy, zz, alpha=0.25)\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], s=18, label=\"Class 0\")\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], s=18, label=\"Class 1\")\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139c46ad",
   "metadata": {},
   "source": [
    "# Part A — Perceptron = linear classifier\n",
    "\n",
    "A (hard) perceptron computes:\n",
    "- **weighted sum + bias:**  z = w·x + b\n",
    "- **threshold:** output 1 if z ≥ 0 else 0\n",
    "\n",
    "In 2D, the decision boundary is a **straight line**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb8124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_predict(X, w, b):\n",
    "    z = X @ w + b\n",
    "    return (z >= 0).astype(int)\n",
    "\n",
    "# A linearly separable dataset\n",
    "X_lin, y_lin = make_linearly_separable(n=250, noise=0.25, seed=1)\n",
    "plot_points(X_lin, y_lin, title=\"Linearly separable dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5954bbe4",
   "metadata": {},
   "source": [
    "## A1) Try a perceptron by hand (change w and b)\n",
    "\n",
    "Goal: choose `w` and `b` so the boundary separates the classes.\n",
    "\n",
    "**Tip:** Start with `w = [1, 1]` and adjust `b` to shift the line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TODO: change these ---\n",
    "w = np.array([1.0, 1.0])\n",
    "b = 0.0\n",
    "\n",
    "y_pred = perceptron_predict(X_lin, w, b)\n",
    "acc = (y_pred == y_lin).mean()\n",
    "\n",
    "plot_decision_boundary(lambda Z: perceptron_predict(Z, w, b), X_lin, y_lin,\n",
    "                       title=f\"Perceptron boundary (acc={acc:.3f})\")\n",
    "\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca36a6",
   "metadata": {},
   "source": [
    "## A2) Why XOR fails\n",
    "\n",
    "Now try XOR. No straight line can separate XOR perfectly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2338b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_xor, y_xor = make_xor(n=260, noise=0.18, seed=2)\n",
    "plot_points(X_xor, y_xor, title=\"XOR dataset (not linearly separable)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a perceptron on XOR (you can still change w,b)\n",
    "w = np.array([1.0, 1.0])\n",
    "b = 0.0\n",
    "\n",
    "y_pred = perceptron_predict(X_xor, w, b)\n",
    "acc = (y_pred == y_xor).mean()\n",
    "\n",
    "plot_decision_boundary(lambda Z: perceptron_predict(Z, w, b), X_xor, y_xor,\n",
    "                       title=f\"Perceptron on XOR (acc={acc:.3f})\")\n",
    "\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ca59b",
   "metadata": {},
   "source": [
    "### Short answer (Part A) — write 5–8 sentences\n",
    "1. In your own words, what geometric object is a perceptron decision boundary in 2D?\n",
    "2. Why can’t a single perceptron solve XOR?\n",
    "3. What kind of change is needed to solve XOR?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89296768",
   "metadata": {},
   "source": [
    "**Your answer here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca1e00",
   "metadata": {},
   "source": [
    "# Part B — “Soft” perceptron with sigmoid\n",
    "\n",
    "Sigmoid activation:\n",
    "σ(z) = 1 / (1 + e^{-z})\n",
    "\n",
    "We interpret σ(z) as a probability-like score.\n",
    "We also introduce **steepness** β:\n",
    "σ(β z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def soft_predict_proba(X, w, b, beta=1.0):\n",
    "    z = beta*(X @ w + b)\n",
    "    return sigmoid(z)\n",
    "\n",
    "zs = np.linspace(-8, 8, 400)\n",
    "plt.figure(figsize=(6.5,4))\n",
    "for beta in [0.5, 1.0, 3.0, 8.0]:\n",
    "    plt.plot(zs, sigmoid(beta*zs), label=f\"beta={beta}\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"sigma(beta z)\")\n",
    "plt.title(\"Sigmoid activation: changing steepness\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b407de",
   "metadata": {},
   "source": [
    "## B1) Soft boundary (change beta)\n",
    "\n",
    "Try `beta = 0.5, 1, 3, 10`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e927eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1.0, 1.0])\n",
    "b = 0.0\n",
    "beta = 1.0  # TODO\n",
    "\n",
    "proba = soft_predict_proba(X_lin, w, b, beta=beta)\n",
    "y_pred = (proba >= 0.5).astype(int)\n",
    "acc = (y_pred == y_lin).mean()\n",
    "\n",
    "plot_decision_boundary(lambda Z: (soft_predict_proba(Z, w, b, beta=beta) >= 0.5).astype(int),\n",
    "                       X_lin, y_lin,\n",
    "                       title=f\"Soft perceptron (beta={beta}) | acc={acc:.3f}\")\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Example probabilities:\", np.round(proba[:10], 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8fc47e",
   "metadata": {},
   "source": [
    "### Short answer (Part B) — write 4–7 sentences\n",
    "1. How does increasing β change the sigmoid?\n",
    "2. Why is a soft output useful?\n",
    "3. Does a soft perceptron solve XOR? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195b5c9",
   "metadata": {},
   "source": [
    "**Your answer here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec72b0b9",
   "metadata": {},
   "source": [
    "# Part C — A small neural network learns nonlinear boundaries (MLP)\n",
    "\n",
    "With a hidden layer + nonlinear activation, a network can represent nonlinear boundaries.\n",
    "We use `sklearn` so we focus on concepts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e848fc75",
   "metadata": {},
   "source": [
    "## C0) Import scikit-learn (run)\n",
    "\n",
    "If this cell errors, install scikit-learn (see setup cell).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8156168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5406c",
   "metadata": {},
   "source": [
    "## C1) Train an MLP on XOR\n",
    "\n",
    "Try:\n",
    "- hidden_layer_sizes = (2,), (10,), (50,)\n",
    "- activation = 'tanh', 'relu', 'logistic'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42176f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure XOR data exists (in case you run cells out of order)\n",
    "try:\n",
    "    X_xor, y_xor\n",
    "except NameError:\n",
    "    X_xor, y_xor = make_xor(n=260, noise=0.18, seed=2)\n",
    "    print(\"Note: XOR dataset was (re)created in this cell.\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_xor, y_xor, test_size=0.3, random_state=0, stratify=y_xor\n",
    ")\n",
    "\n",
    "hidden_layer_sizes = (10,)   # TODO: try (2,), (10,), (50,)\n",
    "activation = \"tanh\"          # TODO: try \"tanh\", \"relu\", \"logistic\"\n",
    "alpha = 1e-4                 # regularization (bigger = simpler)\n",
    "max_iter = 4000\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=hidden_layer_sizes,\n",
    "    activation=activation,\n",
    "    alpha=alpha,\n",
    "    random_state=0,\n",
    "    max_iter=max_iter,\n",
    ")\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "train_acc = mlp.score(X_train, y_train)\n",
    "test_acc = mlp.score(X_test, y_test)\n",
    "\n",
    "print(\"Train accuracy:\", train_acc)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "\n",
    "title = (\n",
    "    f\"MLP on XOR | hidden={hidden_layer_sizes}, act={activation}\\n\"\n",
    "    f\"train={train_acc:.3f}, test={test_acc:.3f}\"\n",
    ")\n",
    "\n",
    "plot_decision_boundary(lambda Z: mlp.predict(Z), X_xor, y_xor, title=title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65df992",
   "metadata": {},
   "source": [
    "### Task C2\n",
    "Answer after trying several settings:\n",
    "\n",
    "1. What happens when the network is too small?\n",
    "2. What happens when it is larger?\n",
    "3. How does activation affect the boundary?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e08de0a",
   "metadata": {},
   "source": [
    "**Your answer here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c2069",
   "metadata": {},
   "source": [
    "# Part D — Underfitting vs Overfitting (train/test)\n",
    "\n",
    "We now use a noisier dataset to see generalisation.\n",
    "You will vary model complexity and regularisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b824e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_linearly_separable(n=400, noise=0.55, seed=7)\n",
    "plot_points(X, y, title=\"Noisier dataset (harder classification)\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc57c9e",
   "metadata": {},
   "source": [
    "## D1) Compare a few configurations\n",
    "\n",
    "Interpretation guide:\n",
    "- **Underfitting:** train low, test low\n",
    "- **Good fit:** train good, test good\n",
    "- **Overfitting:** train very high, test worse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1bd596",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {\"hidden\": (2,),  \"alpha\": 1e-2},\n",
    "    {\"hidden\": (10,), \"alpha\": 1e-3},\n",
    "    {\"hidden\": (80,), \"alpha\": 1e-6},\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    model = MLPClassifier(hidden_layer_sizes=cfg[\"hidden\"],\n",
    "                          activation=\"tanh\",\n",
    "                          alpha=cfg[\"alpha\"],\n",
    "                          random_state=0,\n",
    "                          max_iter=5000)\n",
    "    model.fit(X_train, y_train)\n",
    "    tr = model.score(X_train, y_train)\n",
    "    te = model.score(X_test, y_test)\n",
    "    print(f\"hidden={cfg['hidden']}, alpha={cfg['alpha']:>8g} | train={tr:.3f}, test={te:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e314b",
   "metadata": {},
   "source": [
    "## D2) Plot one chosen configuration\n",
    "\n",
    "Choose values for `hidden_layer_sizes` and `alpha` and plot the boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77979180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose values for hidden_layer_sizes and alpha and plot the boundary.\n",
    "\n",
    "hidden_layer_sizes = (10,)  # TODO: try (2,), (10,), (80,)\n",
    "alpha = 1e-3                # TODO: try 1e-2, 1e-3, 1e-6\n",
    "\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=hidden_layer_sizes,\n",
    "    activation=\"tanh\",\n",
    "    alpha=alpha,\n",
    "    random_state=0,\n",
    "    max_iter=5000,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "train_acc = model.score(X_train, y_train)\n",
    "test_acc = model.score(X_test, y_test)\n",
    "\n",
    "title = (\n",
    "    f\"Boundary | hidden={hidden_layer_sizes}, alpha={alpha}\\n\"\n",
    "    f\"train={train_acc:.3f}, test={test_acc:.3f}\"\n",
    ")\n",
    "\n",
    "plot_decision_boundary(lambda Z: model.predict(Z), X, y, title=title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21955525",
   "metadata": {},
   "source": [
    "### Short answer (Part D) — write 6–10 sentences\n",
    "1. Give one configuration that underfits and explain how you know.\n",
    "2. Give one configuration that overfits and explain how you know.\n",
    "3. Give at least two practical strategies to reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bdcbbd",
   "metadata": {},
   "source": [
    "**Your answer here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe38690",
   "metadata": {},
   "source": [
    "# Wrap-up reflection (10 minutes)\n",
    "\n",
    "Write 6–10 sentences:\n",
    "1. Why do neural networks need nonlinear activation functions?\n",
    "2. What is the most important limitation of a single perceptron?\n",
    "3. Why do we use a train/test split?\n",
    "4. (Optional) How does this connect to complex systems (nonlinearity, emergence, etc.)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f6425",
   "metadata": {},
   "source": [
    "**Your answer here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971b6952",
   "metadata": {},
   "source": [
    "---\n",
    "## Troubleshooting\n",
    "\n",
    "- If plots do not appear in VS Code, try running the notebook in your browser (Jupyter Notebook).\n",
    "- If MLP does not converge, increase `max_iter` or increase `alpha` slightly.\n",
    "- If `sklearn` is missing, run: `pip install scikit-learn`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
